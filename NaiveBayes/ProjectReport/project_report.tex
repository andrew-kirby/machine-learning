\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}


\jmlrheading{1}{2019}{INSERT PAGE STUFF}{9/19}{DATE PUBLISHED}{PAPER ID}{Andrew Kirby, Kevin Browder, Nathan Stouffer, Eric Kempf}

% TODO insert page information when done
% TODO figure out date published
% TODO figure out paper id

\begin{document}

\title{Learning with Naive Bayes}

\author{\name Andrew Kirby \email andy-kirby@live.com \AND
		\name Kevin Browder \email browderkevin54@gmail.com \AND
		\name Nathan Stouffer \email nathanstouffer1999@gmail.com \AND
		\name Eric Kempf \email erickempf123@gmail.com }

\editor{}

% TODO figure out editor
	
\maketitle

\begin{abstract}
	
\end{abstract}

\begin{keywords}
	% TODO insert 5 keywords
\end{keywords}

\section{Introduction}

\section{Problem Statement}

The problem that we must solve is a classification problem. Given an input file that contains examples (each example consists of a list of attributes and an associated classification), our task is to implement a learning algorithm that is trained to classify examples. The algorithm we will implement is called Naive Bayes. The performance of our learning algorithm will be evaluated by two metrics of our choosing and 10-fold cross validation.
% TODO insert metrics that we chose
When we have implemented the algorithm, we then perform our experiment. We are tasked with testing whether scrambling values in $10\%$ of the features will affect the performance of Naive Bayes. This effectively eliminates the usefulness of $10\%$ of features in a given data set.

\subsection{Hypothesis}

We predict that scrambling $10\%$ of features will marginally affect performance.

\section{Algorithm}

\paragraph{Naive Bayes}Naive Bayes is a low cost classifying algorithm that utilizes a probabilistic model based in Bayesian Decision Theory \citep{nbPaper:2014}. Given an example $x \in X$ with attributes $a_1, a_2, ..., a_d$ and belonging to class $c \in C$, the algorithm will classify $x$ by choosing the maximum probability $P(c|a_1, a_2, ..., a_d)$ from all $c \in C$.

\paragraph{Using the probability}Calculating this probability becomes more apparent when rewritten using Bayes Theorem:

$$P(c|a_1, a_2, ..., a_d) = \frac{P(a_1, a_2, ..., a_d | c)P(c)}{P(a_1, a_2, ..., a_d)} $$

\paragraph{} Because the algorithm chooses a classification $c = argmax_{c \in C}P(c|a_1, a_2, ..., a_d)$, the classification is still equivalent to

\section{Experimental Design}

\subsection{Set Up}

% TODO insert issues with input files
% TODO scrambling

\subsection{Tuning}

Add stuff about bin size, what else? \newline
Experimenting w/ using different attributes

\subsection{Final Parameters}

-Bin size \newline
-Number of attributes used \newline
-Which attributes used? \newline

\section{Results}

\section{Summary}

\bibliography{nbBib}

\end{document}