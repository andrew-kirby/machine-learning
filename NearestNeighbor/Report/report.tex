\documentclass[twoside,11pt]{article}

\usepackage{jmlr2e}
\usepackage{spverbatim}

\begin{document}

\title{Learning with Nearest Neighbor}

\author{\name Andrew Kirby \email andy-kirby@live.com \AND
		\name Kevin Browder \email browderkevin54@gmail.com \AND
		\name Nathan Stouffer \email nathanstouffer1999@gmail.com \AND
		\name Eric Kempf \email erickempf123@gmail.com }

\maketitle

\begin{abstract}
	blah Abstract deez nuutz
	
	Andy 
	
\end{abstract}

\section{Problem Statement w/ Hypothesis}

Given an input file that contains examples, the task main is to implement the K Nearest Neighbors learning algorithm for both classification and regression problems. Given that same input file a secondary task is to reduce the size of the data set by implementing the Edited K-Nearest Neighbor, Condensed Nearest Neighbor, K-Means and K-Medoids data reducing algorithms. Of particular are interst are how the data reducers contribute to the preformance of the K-Nearest Neighbor classifier and regressor.

\section{Algorithms}

\subsection{K-NN}

ANDY

\subsection{Distance Metrics}

nn assumes there is a distance metric

\subsection{Data Reducers}
The K-Nearest Neighbor decision rule is a lazy algorithm where the performance is a function of the number of training examples. Therefore, in terms of a learner's computational efficiency, the most favorable training set is the smallest set.
This is subject to the constraint that the model's performance is not compromised.

Thus, the optimal training set for a K-NN learner is the smallest set that is able to correctly identify any query example.
The purpose of the following four algorithms is to identify and remove unnecessary examples from the training set. Since K-NN is lazy by nature, this will improve the computation time of a learner.
% should I talk about not decreasing performance? That is what we are testing right


\subsubsection{Edited K-NN}
The Edited K-Nearest Neighbor algorithm begins the discussion of algorithms that reduce the size of a training set. Edited K-NN begins with an initial set of training examples $D$ and classifies each example $e \in D$ using a K-NN algorithm. For each example $e$, K-NN returns the predicted class of $e$: $predclass(e)$. However, recall that $e$ is a training example, so the actual class of $e$, $class(e)$ is known. So, for each $e$ where $class(e) \neq predclass(e)$ (if $e$ was misclassified), the algorithm removes $e$ from $D$.

This creates a new set $D^\prime$. 

As a side note, since multiple steps of the algorithm relies on knowing the classification of $e$, Edited K-NN can only be used on classification problems.
% TODO talk about the k used in edited k-nn

% explain edited (cite paper)
% metric for decisions (accuracy)

% notes: edited paper has a good line of reasoning for the hypothesis
% nn-d2l has information for tuning
% remember to discuss lack of knowledge about overfitting in convergence based algorithms
% TODO get rid of K-NN in reducing algorithm titles

\subsubsection{Condensed K-NN}

Eric

\subsubsection{Kmeans K-NN}

Eric test

\subsubsection{PAM K-NN}
The Partitioning Around Medoids (PAM) K-NN is a variant of K-Means K-NN but is more resistant to outliers because it uses an actual point in the dataset instead of making a new point using means. PAM begins by randomly selecting $C$ number of data points from the data set $D$ as the medoids $m$. In this case $C$ is determined by the number of points returned by Edited K-NN. The algorithm then clusters all of the remaining examples $x$ to the closest corresponding $m$. Then the distortion is calculated with $D = \sum_{j=1}^{k}\sum_{i\epsilon cluster_j} (x_i - m_j)$. We then iterate through each $m$ and swap it with every $x \epsilon D x \notin m$. After every swap the distortion is calculated again. This was optimized by calculating the distortion of each medoid separately and only recalculating the distortion for the medoid that was changed. If the new distortion is less than the original distortion the algorithm moves on and if the new distortion is greater than the original distortion than the medoid and example are swapped back. The clusters are then cleared and the algorithm then recalculates all of the clusters and distortion and repeats the swapping until no more swapping occurs or the algorithm iterates through 1000 times. 

\section{Experiment}

\subsection{Preprocessing Choices}

All of the preprocessing is done in Python and outputted in .csv files for the main Java program to read in. First all the examples in the dataset are randomly scrambled and then assigned to sets for ten-fold cross validation. All categorical variables are converted to integers and the preprocessor also generates a similarity matrix for each categorical variable that is used for determining distanced between categorical variables. Class names are also converted to incrementing integers for easier reading by the Java program. All numerical variables are normalized between 0 and 1. The preprocessor generates a three lines of key information that the Java program needs to run and adds this data to the beginning of the .csv file. The similarity matrices are outputted next followed by the examples. Having Python do most of these calculations greatly simplified our Java program and made preprocessing much simpler. The data sets did not contain any missing variables so preprocessing did not need to handle that. 

\subsection{Tuning}

A few of us

\section{Results}

a few of us

\section{Summary}



\bibliography{report}

\end{document}