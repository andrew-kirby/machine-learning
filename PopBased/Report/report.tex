%\documentclass[twoside,11pt]{article}
\documentclass[twoside,10pt]{article}

\usepackage{jmlr2e}
\usepackage{spverbatim}
\usepackage{amsmath}
\usepackage{caption}

\newcommand{\ith}{$i^{th}$ }
\newcommand{\jth}{$j^{th}$ }
\newcommand{\weight}{$w_{ji}$ }
\newcommand{\Rw}{\mathbb{R}^w }

\begin{document}

\title{Learning with Neural Networks using Population Based Algorithms}

\author{\name Andrew Kirby \email andy-kirby@live.com \AND
		\name Kevin Browder \email browderkevin54@gmail.com \AND
		\name Nathan Stouffer \email nathanstouffer1999@gmail.com \AND
		\name Eric Kempf \email erickempf123@gmail.com }

\maketitle

\begin{abstract}

\end{abstract}

% TODO make sure to abbreviate PBA and algorithm names

\section{Problem Statement}
	Given datasets, the task is to find a model that predicts a real value or class based upon a feature set. The abalone dataset contains features paired with the age of an abalone (the class). This set has 28 closely related classes, making classification potentially difficult.
	The car dataset contains attributes corresponding to 4 possible conditions of a car while the segmentation dataset details 6 subjects of outdoor photographs with sets of attributes.
	The rest of the datasets are regression.
	The forest fires set contains the area burned by a forest fire. Most of the values are 0 with a few large values which might give the model trouble.
	The machine dataset predicts the performance of a CPU which has an even distribution across its regression values. Wine quality contains the score of a wine with associated attributes. Several of the attributes are correlated, making regression more difficult \citep{datasets}.

\subsection{Hypothesis}

\section{Algorithms}

	Population Based Algorithms typically reference individual members of populations as vectors associated with a fitness value. Each element in a vector corresponds to some value in a model that is being trained by the PBA.
	
	Recall that the output of a Neural Network is entirely dependent on the values of the weights within in the network. 
	So a neural network can be converted to vector form by listing its weights.
	Since weights are real-valued, if $w$ represents the number of weights that a neural network contains then the vector is a point in the Euclidean Space $\mathbb{R}^w$. 
	The fitness associated with the vector is the accuracy or MSE (respectively for classification and regression) of a network on a training data set. 
	
	This sets up the search space of weights and the objective function based on the network's fitness.

\subsection{Genetic Algorithm}

\subsection{Differential Evolution}
Differential Evolution (DE) is another evolution inspired algorithm with  similarities to GA because it uses crossover and mutation.  
\subsection{Particle Swarm Optimization}

	PSO differs from the GA and DE in that there is no parent-offspring relationship. Instead, PSO consists of particles that interact and move around the search space to find the optimum solution. 
	
	A swarm consists of $N$ individuals each with a position $\vec{x} \in \Rw$ and a fitness value. 
	PSO then uses an iterative update rule to change each individual's position. The update for an individual is named $\vec{v} \in \Rw$, which is composed of 3 summed components.
	
	The first component is inertia. For a given iteration's velocity $\vec{v}^t$ inertia is given as $\omega * \vec{v}^{t-1}$ where $\omega$ is typically in $(0,1)$ \citep{empirical-pso}.

\section{Experiment}

\subsection{Preprocessing Choices}

	First, all the examples in the data set are randomly scrambled and then assigned to sets for ten-fold cross validation. 
	All categorical variables are converted to integers 
	and the preprocessor also generates a similarity matrix for each categorical variable that is used for
	determining distances between categorical variables. 
	All numerical variables are normalized
	between 0 and 1. The data sets did not contain any missing variables.

\subsection{Evaluation Metrics}

	Classification datasets are evaluated with accuracy and mean squared error (MSE). 
	The MSE metric implemented measures the squared error between the predicted and actual class distributions. 
	This is similar in concept to a Brier score but slightly different in computation. Accuracy indicates how well the algorithm individually classifies examples.
	
	To evaluate regression datasets, mean error (ME) and MSE will be used. MSE squares the distance between a real and predicted value, the squares are then averaged over the entire testing set. 
	ME is computed similarly, but will not square the difference. MSE emphasizes the effect of outliers while ME captures whether the learner tends to over or underestimate the values in the test set. 
	MSE and ME are computed using z-scores (the number of standard deviations from the mean) so that comparisons can be made between datasets.

\subsection{Tuning}

\section{Results}

\section{Summary}

\newpage

\bibliography{biblio}

\end{document}
