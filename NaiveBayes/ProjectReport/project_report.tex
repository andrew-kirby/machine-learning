\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}


\jmlrheading{1}{2019}{INSERT PAGE STUFF}{9/19}{DATE PUBLISHED}{PAPER ID}{Andrew Kirby, Kevin Browder, Nathan Stouffer, Eric Kempf}

% TODO insert page information when done
% TODO figure out date published
% TODO figure out paper id

\begin{document}

\title{Learning with Naive Bayes}

\author{\name Andrew Kirby \email andy-kirby@live.com \AND
		\name Kevin Browder \email browderkevin54@gmail.com \AND
		\name Nathan Stouffer \email nathanstouffer1999@gmail.com \AND
		\name Eric Kempf \email erickempf123@gmail.com }

\editor{}

% TODO figure out editor
	
\maketitle

\begin{abstract}
	
\end{abstract}

\begin{keywords}
	% TODO insert 5 keywords
\end{keywords}

\section{Introduction}

Naive Bayes is a simple algorithm and a nice introduction into machine learning. It works surprisingly well at classification learning. A classification algorithm takes a given set of classes and attributes, and determines the probability of that a given class has said attributes. The reason Naive Bayes is so simple is due to the very powerful assumption that our attribute set is conditionally independent. This assumption, although probably untrue for most sets, is what allows the Naive Bayes classifier to do what it does, and do it relatively well for how simple it is. In this paper we go over our process of training the Naive Bayes classifier on various data sets, and the results of each trial. \\\\
The paper is organized as follows. Section 2 goes over the problem we had to solve. Section 3 takes a look at the actual classification algorithm. Section 4 goes over our design process and each step we took to get our end results.

\section{Problem Statement}

+The problem is a classification problem. Given an input file that contains examples (each example consists of a list of attributes and an associated classification), the task is to implement a learning algorithm that is trained to classify examples. The algorithm is called Naive Bayes. The performance of the learning algorithm will be evaluated by two metrics and 10-fold cross validation. Specifically, the metrics are accuracy and mean squared error. 

\subsection{Variables}

The independent variable is whether the data is scrambled or not. Scrambling is described as follows. First, $10\%$ of the attributes in a given data set are randomly selected. Then, within each attribute, the values are randomly swapped between examples. Now the data set is scrambled. The dependent variable is how well the algorithm performs.

\subsection{Hypothesis}

The hypothesis is that scrambling a given data set will not significantly change the performance of the Naive Bayes Algorithm. In essence, scrambling renders $10\%$ of attributes useless. Any pattern that existed before scrambling is no longer discernible within those attributes. However, there remains $90\%$ of the data that persists with the original pattern. Naive Bayes chooses a classification based on the relative probability that a given example is in a class. The order of the relative probabilities will stay constant even when the original pattern is lost in $10\%$ of the attributes.

\section{Algorithm}

\paragraph{Naive Bayes}Naive Bayes is a low cost classifying algorithm that utilizes a probabilistic model based in Bayesian Decision Theory \citep{nbPaper:2014}. Given an example $x \in X$ with attributes $a_1, a_2, ..., a_d$ and belonging to class $c \in C$, the algorithm will classify $x$ by choosing the maximum probability $P(c|a_1, a_2, ..., a_d)$ from all $c \in C$.

\paragraph{Using the probability}Calculating this probability becomes more apparent when rewritten using Bayes Theorem:

$$P(c|a_1, a_2, ..., a_d) = \frac{P(a_1, a_2, ..., a_d | c)P(c)}{P(a_1, a_2, ..., a_d)} $$

\paragraph{} Because the algorithm chooses a classification $c = argmax_{c \in C}P(c|a_1, a_2, ..., a_d)$, the classification is still equivalent to

\section{Experimental Design}

\subsection{Set Up}

% TODO insert issues with input files
% TODO scrambling

\subsection{Tuning}

Add stuff about bin size, what else? \newline
Experimenting w/ using different attributes

\subsection{Final Parameters}

-Bin size \newline
-Number of attributes used \newline
-Which attributes used? \newline

\section{Results}

\section{Summary}

\bibliography{nbBib}

\end{document}