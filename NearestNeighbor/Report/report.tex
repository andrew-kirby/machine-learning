\documentclass[twoside,11pt]{article}

\usepackage{jmlr2e}
\usepackage{spverbatim}

\begin{document}

\title{Learning with Nearest Neighbor}

\author{\name Andrew Kirby \email andy-kirby@live.com \AND
		\name Kevin Browder \email browderkevin54@gmail.com \AND
		\name Nathan Stouffer \email nathanstouffer1999@gmail.com \AND
		\name Eric Kempf \email erickempf123@gmail.com }

\maketitle

\begin{abstract}
	blah Abstract deez nuutz
	
	we learned the relative performances of these algs
	knn generally better, tradeoff performance for less data storage with other algs
	
	Andy 
	
\end{abstract}

\section{Problem Statement w/ Hypothesis}

Given an input file that contains examples, the task main is to implement the K Nearest Neighbors learning algorithm for both classification and regression problems. Given that same input file a secondary task is to reduce the size of the data set by implementing the Edited K-Nearest Neighbor, Condensed Nearest Neighbor, K-Means and K-Medoids data reducing algorithms. Of particular are interst are how the data reducers contribute to the preformance of the K-Nearest Neighbor classifier and regressor.


Andy's word dump to go somewhere in intro (will make prettier) 

parametric learning is nice because it has a fixed set of parameters/fixed form (ex: Naive Bayes) that can be learned quickly and understood simply. They dont require as much data to train and are fast.

The limitation of these forms of learning is that its functional form constrains the performance.

The data sets to be handled do not follow an unknown distribution/model. Thus, constructing parametric learners for these sets is very hard.

This is where nonparametric comes along. Nonparametric does not construct a model with a fixed form. It makes no strong assumptions about the form of the data.

One popular form of nonparametric learning is K-NN, which will be used to predict classes and values 

\section{Algorithms}

\subsection{K-NN}

K Nearest Neighbor (K-NN) is a modification of the nearest neighbor rule, a nonparametric procedure for classification or regression that utilizes a set of known points.

At its core, the nearest neighbor rule utilizes the intuitive assumption that a set of known examples, $X$, follows the distribution of all possible examples that it was taken from. As such, examples which are ``close" to each other likely belong to the same class or have a similar real value (\cite{NNClassification}). Metrics for determining this distance are discussed in section 2.2.

Thus, given $X$, a number of neighbors $k$, and a new example $x$, K-NN finds the $k$ nearest neighbors of $x$ by iterating through the examples in $X$, calculating the distance from $x$ for each example. Then K-NN classifies $x$ given a majority vote among the returned class values from its $k$ nearest neighbors. K-NN handles regression by computing the mean value among its $k$ nearest neighbors.

Note that this does not heavily assume anything about the form of the actual distribution. K-NN is flexible enough to adapt to almost any functional form that the underlying distribution takes on. This is a huge advantage for data sets that are hard to visualize or have unknown traits. The downside to this characteristic is that, like many nonparametric models, K-NN requires much more data than a parametric model to train effectively. In other words, K-NN requires a great number of known examples because it must is essentially estimating the real distribution from the known examples. Without sufficient data, K-NN will perform poorly.

In fact, the error of K-NN, $R_{knn}$ is bounded by:
$$R^* \leq R_{knn} \leq (1+\frac{1}{k})R^*$$ 
where $R^*$ is the Bayes risk, the optimal risk given a known set of probability densities that represents the data (\cite{NNClassification}). Thus, as $k \rightarrow \infty$ and the number of data points $n \rightarrow \infty$, theoretically $R_{knn} \rightarrow R^*$. Of course, $\frac{k}{n} \rightarrow 0$ should be ensured so that $k$ is still only a small subset of the total data points $n$. In short, K-NN will work optimally as the number of points sampled approaches infinity. As more attributes or data complexity is added, K-NN will approach its optimal condition slower given increasing $n$.


\subsection{Distance Metrics}

The nearest neighbor rule, and therefore K-NN, requires a distance metric to determine the theoretical ``distance" between two examples. A common distance metric---and the one used in this paper---is Euclidean distance. The Euclidean distance $D$ between two examples $x_1$ and $x_2$ is computed as:
$$D = \sqrt{\sum_{i=0}^{d}(a_1^i - a_2^i)^2}$$
where $d$ is the number of attributes the examples have, $a_1^i$ is the $i$-th attribute of $x_1$, and $a_2^i$ is the $i$-th attribute of $x_2$.

\subsection{Data Reducers}
The K-Nearest Neighbor decision rule is a lazy algorithm where the performance is a function of the number of training examples. Therefore, in terms of a learner's computational efficiency, the most favorable training set is the smallest set.
This is subject to the constraint that the model's performance is not compromised.

Thus, the optimal training set for a K-NN learner is the smallest set that is able to maximize correct identification of any query example.
The purpose of the following four algorithms is to identify and remove unnecessary examples from the training set. Since K-NN is lazy by nature, this will improve the computation time of the learner.
% should I talk about not decreasing performance? That is what we are testing right


\subsubsection{Edited K-NN}
The Edited K-Nearest Neighbor algorithm begins the discussion of algorithms that reduce the size of a training set. Edited K-NN begins with an initial set of training examples $D$ and classifies each example $e \in D$ using a K-NN algorithm. For each example $e$, K-NN returns the predicted class of $e$: $predclass(e)$. However, recall that $e$ is a training example, so the actual class of $e$, $class(e)$ is known. So, for each $e$ where $class(e) \neq predclass(e)$ (if $e$ was misclassified), the algorithm removes $e$ from $D$.

This creates a new set $D^\prime$. 

As a side note, since multiple steps of the algorithm relies on knowing the classification of $e$, Edited K-NN can only be used on classification problems.
% TODO talk about the k used in edited k-nn

% explain edited (cite paper)
% metric for decisions (accuracy)

% notes: edited paper has a good line of reasoning for the hypothesis
% nn-d2l has information for tuning
% remember to discuss lack of knowledge about overfitting in convergence based algorithms
% TODO get rid of K-NN in reducing algorithm titles

\subsubsection{Condensed K-NN}

Eric

\subsubsection{Kmeans K-NN}

Eric test

\subsubsection{PAM K-NN}
The Partitioning Around Medoids (PAM) K-NN is a variant of K-Means K-NN but is more resistant to outliers because it uses an actual point in the dataset instead of making a new point using means. PAM begins by randomly selecting $C$ number of data points from the data set $D$ as the medoids $m$. In this case $C$ is determined by the number of points returned by Edited K-NN. The algorithm then clusters all of the remaining examples $x$ to the closest corresponding $m_i \in m$. Then the distortion is calculated with $$D = \sum_{j=1}^{k}\sum_{i \in cluster_j} (x_i - m_j)$$. We then iterate through each $m$ and swap it with every $x_j \in D$ $x_j \notin m$. After every swap the distortion $D$ is calculated again. This was optimized by calculating the distortion of each medoid separately and only recalculating the distortion for the medoid that was changed. If the new distortion is less than the original distortion the algorithm moves on and if the new distortion is greater than the original distortion than the medoid and example are swapped back. The clusters are then cleared and the algorithm then recalculates all of the clusters and distortion and repeats the swapping until no more swapping occurs or the algorithm iterates through 1000 times. 

\section{Experiment}

\subsection{Preprocessing Choices}

All of the preprocessing is done in Python and outputted in .csv files for the main Java program to read in. First all the examples in the dataset are randomly scrambled and then assigned to sets for ten-fold cross validation. All categorical variables are converted to integers and the preprocessor also generates a similarity matrix for each categorical variable that is used for determining distanced between categorical variables. Class names are also converted to incrementing integers for easier reading by the Java program. All numerical variables are normalized between 0 and 1. The preprocessor generates a three lines of key information that the Java program needs to run and adds this data to the beginning of the .csv file. The similarity matrices are outputted next followed by the examples. Having Python do most of these calculations greatly simplified our Java program and made preprocessing much simpler. The data sets did not contain any missing variables so preprocessing did not need to handle that. 

\subsection{Tuning}

A few of us

\section{Results}

a few of us

A FULL PAGE (OR MORE) WITH GRAPHS OF EVAL METRICS for each data set. CORRESPONDING GRAPHS SIDE BY SIDE WITH K varied. SIDE BY SIDE WITH C varied.

if kevin didnt mention normalization in preprocessing, we can talk about it here?

\section{Summary}



\bibliography{report}

\end{document}