\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}

\begin{document}

\title{Learning with Naive Bayes}

\author{\name Andrew Kirby \email andy-kirby@live.com \AND
		\name Kevin Browder \email browderkevin54@gmail.com \AND
		\name Nathan Stouffer \email nathanstouffer1999@gmail.com \AND
		\name Eric Kempf \email erickempf123@gmail.com }
	
\maketitle

\begin{abstract}
	
\end{abstract}

\section{Introduction}

Naive Bayes is a simple algorithm and a nice introduction into machine learning. It works surprisingly well at classification learning. A classification algorithm takes a given set of classes and attributes, and determines the probability of that a given class has said attributes. The reason Naive Bayes is so simple is due to the very powerful assumption that our attribute set is conditionally independent. This assumption, although probably untrue for most sets, is what allows the Naive Bayes classifier to do what it does, and do it relatively well for how simple it is. In this paper we go over our process of training the Naive Bayes classifier on various data sets, and the results of each trial. \\\\
The paper is organized as follows. Section 2 goes over the problem we had to solve. Section 3 takes a look at the actual classification algorithm. Section 4 goes over our design process and each step we took to get our end results.

\section{Problem Statement}

The problem is a classification problem. Given an input file that contains examples (each example consists of a list of attributes and an associated classification), the task is to implement a learning algorithm that is trained to classify examples. The algorithm is called Naive Bayes. The performance of the learning algorithm will be evaluated by two metrics and 10-fold cross validation. Specifically, the metrics are accuracy and mean squared error. 

\subsection{Variables}

The independent variable is whether the data is scrambled or not. Scrambling is described as follows. First, $10\%$ of the attributes in a given data set are randomly selected. Then, within each attribute, the values are randomly swapped between examples. Now the data set is scrambled. The dependent variable is how well the algorithm performs.

\subsection{Hypothesis}

The hypothesis is that scrambling a given data set will not significantly change the performance of the Naive Bayes Algorithm. In essence, scrambling renders $10\%$ of attributes useless. Any pattern that existed before scrambling is no longer discernible within those attributes. However, there remains $90\%$ of the data that persists with the original pattern. Naive Bayes chooses a classification based on the relative probability that a given example is in a class. The order of the relative probabilities will stay constant even when the original pattern is lost in $10\%$ of the attributes.

\section{Algorithm}

Naive Bayes is a low cost classifying algorithm that utilizes a probabilistic model based in Bayesian Decision Theory \citep{nbPaper:2014}. Given an example $x \in X$ with attributes $a_1, a_2, ..., a_d$ and belonging to class $c \in C$, the algorithm will classify $x$ by choosing the maximum probability $P(c|a_1, a_2, ..., a_d)$ from all $c \in C$.

Calculating this probability becomes more apparent when rewritten using Bayes Theorem:
$$P(c|a_1, a_2, ..., a_d) = \frac{P(a_1, a_2, ..., a_d | c)P(c)}{P(a_1, a_2, ..., a_d)} $$

Because the algorithm chooses a classification $c = argmax_{c \in C}P(c|a_1, a_2, ..., a_d)$, the classification is equivalent to
$$c = argmax_{c \in C}P(a_1, a_2, ..., a_d | c)P(c)$$

Naive Bayes assumes all attributes $a \in A$ are conditionally independent given the class, simplifying the classification decision to 
$$c = argmax_{c \in C}P(c)\sum_{i = 1}^{d}P(a_i | c)$$

These probabilities are calculated when the algorithm is trained with a given set of examples $X_{test} \subset X$. During training, $P(c)$, is calculated for all classes by
$$P(c) = \frac{|x \in c|}{|X_{test}|}$$
and $P(a_i | c)$ is calculated with
$$P(a_i | c) = \frac{|(x \in c) \cap (x_{a_i} = a_i)| + 1}{|x \in c| + d}$$
where $d$ is the number of examples in the class. 

	Once the algorithm has been trained, all probabilities $P(c)$ and $P(a_i | c)$ have been calculated and stored for future reference. Classification of a new example will be selected quickly with little further computation.

\section{Experimental Design}

\subsection{Pre-processing}

The pre-processing is done using the pandas library in Python. The class column is moved before all of the attributes for a consistent output between datasets. Next the examples are randomly shuffled. A new column is added after the class column and each example is assigned to one of 10 sets that are used in the 10 fold cross validation. If the data is continuous, it is discretized into 5 bins with each bin containing an equal number of examples. Non numerical data is changed to integers because the algorithm can only read integers, this includes the class names. Before output three lines of data are generated, the first line includes the number of classes, number of attributes and the number of examples. The second line includes the number of bins for each attribute. The third line include the class names so the algorithm can convert back from numerical class names to the original string names. After these lines are generated they are outputted to the first three lines of the .csv file and the pre-processed data is outputted starting on the fourth line. After this data is outputted ten percent of the attributes are randomly chosen and the data in each of these attributes is randomly shuffled. This shuffled data is then outputted to a new .csv with the same header as the first file.  

\subsection{Tuning}

Add stuff about bin size, what else? \newline
Experimenting w/ using different attributes

\subsection{Final Parameters}

-Bin size \newline
-Number of attributes used \newline
-Which attributes used? \newline

\section{Results}



\begin{table}[h]
	\centering
	\caption{Loss Function Metrics} \label{tab:metrics}
	\begin{tabular}{|l|l|l|}
		\hline
		Dataset                  & Accuracy & MSE  \\ \hline
		glass                    & 80.78\%  & 2.9  \\ \hline
		glass-scrambled          & 78.03\%  & 3.17 \\ \hline
		iris                     & 92.67\%  & 0.73 \\ \hline
		iris-scrambled           & 90.67\%  & 1    \\ \hline
		house-votes-84           & 90.13\%  & 6.1  \\ \hline
		house-votes-84-scrambled & 89.90\%  & 4.2  \\ \hline
		soybean-small            & 79.50\%  & 0.9  \\ \hline
		soybean-small-scrambled  & 79.50\%  & 0.9  \\ \hline
		wdbc                     & 94.02\%  & 4    \\ \hline
		wdbc-scrambled           & 93.85\%  & 3.3  \\ \hline
	\end{tabular}
\end{table}

\section{Summary}

\bibliography{nbBib}

% TODO insert these tables into Tuning


\begin{table}[h]
	\centering
	\caption{Loss Function Metrics for bin\_size = 2} \label{tab:metrics2}
	\begin{tabular}{|l|l|l|}
		\hline
		Dataset                  & Accuracy & MSE  \\ \hline
		glass                    & 69.11\%  & 5    \\ \hline
		iris                     & 70.67\%  & 7.8  \\ \hline
		house-votes-84           & 89.66\%  & 4.3  \\ \hline
		soybean-small            & 79\%     & 0.65 \\ \hline
		wdbc                     & 91.92\%  & 7.4  \\ \hline
	\end{tabular}
\end{table}

\begin{table}[h]
	\centering
	\caption{Loss Function Metrics for bin\_size = 5} \label{tab:metrics5}
	\begin{tabular}{|l|l|l|}
		\hline
		Dataset                  & Accuracy & MSE  \\ \hline
		glass                    & 80.78\%  & 2.9  \\ \hline
		iris                     & 92.67\%  & 0.73 \\ \hline
		house-votes-84           & 90.13\%  & 6.1  \\ \hline
		soybean-small            & 79.50\%  & 0.9  \\ \hline
		wdbc                     & 94.02\%  & 4    \\ \hline
	\end{tabular}
\end{table}

\begin{table}[]
	\centering
	\caption{Loss Function Metrics for bin\_size = 1000} \label{tab:metrics1000}
	\begin{tabular}{|l|l|l|}
		\hline
		Dataset                  & Accuracy & MSE    \\ \hline
		glass                    & 5.13\%   & 80.67  \\ \hline
		iris                     & 92.67\%  & 0.87   \\ \hline
		house-votes-84           & 89.66\%  & 7.9    \\ \hline
		soybean-small            & 83.50\%  & 0.6    \\ \hline
		wdbc                     & 39.54\%  & 1191.4 \\ \hline
	\end{tabular}
\end{table}

\end{document}