\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}


\jmlrheading{1}{2019}{INSERT PAGE STUFF}{9/19}{DATE PUBLISHED}{PAPER ID}{Andrew Kirby, Kevin Browder, Nathan Stouffer, Eric Kempf}

% TODO insert page information when done
% TODO figure out date published
% TODO figure out paper id

\begin{document}

\title{Learning with Naive Bayes}

\author{\name Andrew Kirby \email andy-kirby@live.com \AND
		\name Kevin Browder \email browderkevin54@gmail.com \AND
		\name Nathan Stouffer \email nathanstouffer1999@gmail.com \AND
		\name Eric Kempf \email erickempf123@gmail.com }

\editor{Kevin Browder}

% TODO figure out editor
	
\maketitle

\begin{abstract}
	
\end{abstract}

\begin{keywords}
	% TODO insert 5 keywords
\end{keywords}

\section{Introduction}

\section{Problem Statement}

The problem that we must solve is a classification problem. Given an input file that contains examples (each example consists of a list of attributes and an associated classification), our task is to implement a learning algorithm that is trained to classify examples. The algorithm we will implement is called Naive Bayes. The performance of our learning algorithm will be evaluated by two metrics of our choosing and 10-fold cross validation.
% TODO insert metrics that we chose
When we have implemented the algorithm, we then perform our experiment. We are tasked with testing whether scrambling values in $10\%$ of the features will affect the performance of Naive Bayes. This effectively eliminates the usefulness of $10\%$ of features in a given data set.

\subsection{Hypothesis}

We predict that scrambling $10\%$ of features will marginally affect performance.

\section{Algorithm}

Naive Bayes is a low cost classifying algorithm that utilizes a probabilistic model based in Bayesian Decision Theory \citep{nbPaper:2014}. Given an example $x \in X$ with attributes $a_1, a_2, ..., a_d$ and belonging to class $c \in C$, the algorithm will classify $x$ by choosing the maximum probability $P(c|a_1, a_2, ..., a_d)$ from all $c \in C$.

Calculating this probability becomes more apparent when rewritten using Bayes Theorem:
$$P(c|a_1, a_2, ..., a_d) = \frac{P(a_1, a_2, ..., a_d | c)P(c)}{P(a_1, a_2, ..., a_d)} $$

Because the algorithm chooses a classification $c = argmax_{c \in C}P(c|a_1, a_2, ..., a_d)$, the classification is equivalent to
$$c = argmax_{c \in C}P(a_1, a_2, ..., a_d | c)P(c)$$

Naive Bayes assumes all attributes $a \in A$ are conditionally independent given the class, simplifying the classification decision to 
$$c = argmax_{c \in C}P(c)\sum_{i = 1}^{d}P(a_i | e)$$

These probabilities are calculated when the algorithm is trained with a given set of examples $X_{test} \subset X$. During training, $P(c)$, is calculated for all classes by
$$P(c) = \frac{|x \in c|}{|X_{test}|}$$ 

\section{Experimental Design}

\subsection{Pre-processing}
The pre-processing is done using the pandas library in Python. The class column is moved before all of the attributes for a consistent output between datasets. Next the examples are randomly shuffled. A new column is added after the class column and each example is assigned to one of 10 sets that are used in the 10 fold cross validation. If the data is continuous, it is discretized into 5 bins with each bin containing an equal number of examples. Non numerical data is changed to integers because the algorithm can only read integers, this includes the class names. Before output three lines of data are generated, the first line includes the number of classes, number of attributes and the number of examples. The second line includes the number of bins for each attribute. The third line include the class names so the algorithm can convert back from numerical class names to the original string names. After these lines are generated they are outputted to the first three lines of the .csv file and the pre-processed data is outputted starting on the fourth line. After this data is outputted ten percent of the attributes are randomly chosen and the data in each of these attributes is randomly shuffled. This shuffled data is then outputted to a new .csv with the same header as the first file.  
% TODO insert issues with input files
% TODO scrambling

\subsection{Tuning}

Add stuff about bin size, what else? \newline
Experimenting w/ using different attributes

\subsection{Final Parameters}

-Bin size \newline
-Number of attributes used \newline
-Which attributes used? \newline

\section{Results}

\section{Summary}

\bibliography{nbBib}

\end{document}