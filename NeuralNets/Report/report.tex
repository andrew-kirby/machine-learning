\documentclass[twoside,11pt]{article}

\usepackage{jmlr2e}
\usepackage{spverbatim}
\usepackage{amsmath}

\newcommand{\ith}{$i^{th}$ }
\newcommand{\jth}{$j^{th}$ }
\newcommand{\weight}{$w_{ji}$ }

\begin{document}

\title{Learning with Neural Nets}

\author{\name Andrew Kirby \email andy-kirby@live.com \AND
		\name Kevin Browder \email browderkevin54@gmail.com \AND
		\name Nathan Stouffer \email nathanstouffer1999@gmail.com \AND
		\name Eric Kempf \email erickempf123@gmail.com }

\maketitle

\begin{abstract}
yeet
\end{abstract}

\section{Problem Statement}

\subsection*{Hypothesis}
RBF is expected to preform better than MLP because it handles categorical variables where MLP does not take them into account. MLP and RBF on the raw data will out preform the clustering methods because they have more data to train on. This is especially important for the MLP with more layers because more layers require more data to adequately train.
\section{Algorithms}
\subsection{RBF}

\subsection{MLP}
The Multilayer Perceptron (MLP) is a type of feed forward neural network used for classification and regression. \\
\indent At its core, the MLP is made up of an input layer, arbitrary number of hidden layers and an output layer. Each hidden layer can contain an arbitrary number of hidden nodes which is a tuneable parameter, see section 3.2.  When using a sigmoidal activation function an MLP is a universal approximator. The algorithm functions by passing in an example to the input layer and activating nodes in the hidden layers towards the output layer and finally reaching the output layer producing either a class or a regression value. The algorithm is trained using stochastic gradient decent and starts by passing data to the network one example at a time. The data is first run through the network from input layer to output layer. The output is then compared to the expected value and an error is calculated from the difference. This data is then back propagated through the network and the contributing weights are adjusted accordingly. 
%TODO: ADD MATH SHIT	
%TODO: ADD MORE
\subsection{Gradient Descent}
Recall that the performance of a Neural Network can be evaluated with an error function.
Most commonly, this error function is chosen to be the Mean Squared Error of each of the output nodes (applied appropriately to classification vs regression problems).

A given network N performs best when it's error function Err is minimized.
At first, this seems like a simple minimization problem: take the derivative of Err and find the weights that produces the smallest output in Err.
While that would be a good solution for a problem with small dimensions, the dimension of Err is same as the number of weights in the network, which can be a massive number.
This means that the classic optimization used in Calculus will be too computationally expensive to find the global minimum of Err. Thus, another technique must be used.
The technique used while training Neural Networks is Gradient Descent (GD).

Gradient Descent is an iterative process used to find a local minimum of a function. Note that only a local minimum is found, not a global minimum.
This means the performance of the Neural Network trained with GD is not necessarily optimal.
However, what is lost in performance is gained back in training time, as Gradient Descent's search for a local minimum is far more efficient than the standard search for a global minimum.

For a function $f$, GD iteratively uses the following rule for finding a local minimum:
$\Delta x_a = - \eta \dfrac{\partial f}{\partial x_a}$ where $x_a$ is value of the Err function in the $a^{th}$ dimension and $\eta$ is a tuned parameter (also called the learning rate). GD has found a local minimum once all the values of $x_a$ have converged.
Applying this to Err, gives
$$\Delta w_{ji} = - \eta \dfrac{\partial Err}{\partial w_{ji}}$$
where $w_{ji}$ is the weight from the $i^{th}$ to the $j^{th}$ node and $\eta$ is the tuned learning rate. Using the chain rule,
$\dfrac{\partial Err}{\partial w_{ji}} = \dfrac{\partial Err}{\partial net_{j}} * \dfrac{\partial net_j}{\partial w_{ji}}$. Note that $net_j = \sum w_{ji} * x_{ji}$ where $x_{ji}$ is the $i^{th}$ input to the $j^{th}$ node.

Beginning with the simpler of the partial derivatives, $\dfrac{\partial net_j}{\partial w_{ji}} = x_{ji}$.
The other partial derivative depends on two conditions: the activation function of a node and whether the node is in a hidden layer or an output layer. Let $A_j(net_j)$ denote the activation function of the \jth node, whether sigmoidal or linear. 
The value of the partial derivative also depends on the layer type of node $j$. 

Let $\delta _j^o$ denote $\dfrac{\partial Err}{\partial net_{j}}$ for the \jth node in the output layer and $\delta ^h_j$ denote $\dfrac{\partial Err}{\partial net_{j}}$ for the \jth hidden layer. 
Then, 
$$\delta _j^o = -A^\prime _j (net_j) * (d_j - o_j)$$
where $d_j$ is the target of the \jth node. And, 
$$\delta ^h_j = A^\prime _j (net_j) * \sum_{k \in ds(j)} \delta _k * w_{kj}$$ 
where $ds(j)$ returns the deltas of the nodes that the \jth node directly affects (also called the downstream nodes).

This sets up the Backpropagation Rule. This rule begins by computing each $\delta _j^o$ for each node in the output layer. Then update the weights that apply to the output layer according to 
$\Delta w_{ji} = - \eta * \delta _j^o * x_{ji}$.
Now the weights going to the output layer have been updated and the network has the necessary values of $\delta _j$ to begin a recursive call (from the last to the first hidden layer) that updates weights by the following rule
$\Delta w_{ji} = - \eta * \delta _j^h * x_{ji}$ using $\delta _j^h$ 
from above.

A true Gradient Descent method would average $\dfrac{\partial Err}{\partial w_{ji}}$ over the entire training set. However, using Stochastic Gradient Descent (SGD). SGD selects a batch of random examples from the training set and averages $\dfrac{\partial Err}{\partial w_{ji}}$ over that batch. This method, while less stable, runs much faster than true GD.

The final step in GD is to repeat batching and propagating the changes backwards until the weights in the network converge. Once the weights have converged, a local minimum has been found and the network is ready to use.

Another option to with GD is to add a momentum term. For a given iteration 
% TODO finish discussing momentum

\subsection{Distance Metrics}

The nearest neighbor rule, and therefore K-NN, requires a distance metric to determine the theoretical ``distance" between two examples. A common distance metric---and the one used in this paper---is Euclidean distance. The Euclidean distance $D$ between two examples $x_1$ and $x_2$ is computed as:
$$D = \sqrt{\sum_{i=0}^{d}(a_1^i - a_2^i)^2}$$
where $d$ is the number of attributes the examples have, $a_1^i$ is the $i$-th attribute of $x_1$, and $a_2^i$ is the $i$-th attribute of $x_2$.

A Euclidean distance metric assumes that all data in a dataset is continuous. However, in the world of data science and this project, some attributes contain categorical values. One method to compute this distance is the Value Difference Metric (VDM). Note that the VDM relies on classification to compute a distance, so continuous regression values must be discretized by some method.

To compute a distance between two categorical values $x_1, x_2$ within one attribute $a$ using the VDM, find the following for each value:
$$ v_n = \sum _{i = 1}^{k} | \dfrac{N_{1i}}{N_1} - \dfrac{N_{2i}}{N_2} | $$
where $N_{1i}, N_{2i}$ are the number of examples in $a$ that are of the $i^{th}$ class and have values $x_1, x_2$ respectively, and $N_1, N_2$ are the number of examples in $a$ that are of the $i^{th}$ class.

Then compute the difference
$v_2 - v_1$.
This difference is considered to be the distance $v$ between the two categorical values $x_1$ and $x_2$ \citep{vdm}. The squared difference can now be included in the summation when computing Euclidean distance.

\section{Experiment}

\subsection{Preprocessing Choices}
First, all the examples in the data set are randomly scrambled and then assigned to sets
for ten-fold cross validation. All categorical variables are converted to integers and the
preprocessor also generates a similarity matrix for each categorical variable that is used for
determining distances between categorical variables. All numerical variables are normalized
between 0 and 1. The data sets did not contain any missing variables so preprocessing did
not to handle that.
\subsection{Evaluation Metrics}
The algorithms were evaluated using accuracy, mean square error (MSE), and mean error (ME).

For classification, MSE is a good indicator of how far off the predicted distribution is from the actual distribution. Accuracy indicates how well the algorithm is classifying examples on an individual basis.

Mean Error (ME) and MSE will be used to evaluate the regression problems. MSE takes the distance between real and predicted values and squares it. ME is computed similarly, but will not square the difference. MSE shows the existence of outliers because the value will increase a square. ME shows whether the learner is over or under estimating the values in the test set.
\subsection{Data Sets}
\subsection{Tuning}
%TODO: FIX THIS
\subsubsection{RBF}
The first hyper parameter will 
The variance of the clusters in the RBF networks is the last attribute that will be tuned. This will be tuned by modifying k in K Nearest Neighbors (K-NN) which is how variance will be calculated. The number of  neighbors k for the K-NN algorithm will be chosen initially to be the square root of the size of the data set, a common value for k (Shichao 2017). From this initial value k will be tuned both up and down to determine the optimal number of neighbors.
\subsubsection{MLP}
Tuning began with learning rate($\eta$), which is the step size at each iteration in a neural network. The following graphs plot the MSE and accuracy for classification and MSE and ME for regression against a spread of learning rates ranging from $1*10^{-4}$ to $5$ with a starting value of $.1$. The graphs also show performance on different numbers of hidden layers ranging from 0 to 2. \\
\begin{figure}[h]
	\centering
	\includegraphics[width=6in]{LearningRateTuning2MultiplierGraphs.JPG}
\end{figure}

Starting with the feed forward network, the most important attribute to tune will be the number of nodes in each hidden layer. The optimum number of nodes in a hidden layer is somewhere between the number of nodes in the input and output layers (Heaton 2008). Tuning will start with the mean of the number of nodes in the input and output layers for classification problems and half the number of nodes in the input layer for regression. The momentum will also be tuned. Tuning of momentum will start at .5 as it is a common value to start tuning with (Goodfellow 2016). From the starting value we will tune downwards to find a more optimal local minimum. Lastly the learning rate will be tuned. Since the input data is normalized between 0 and 1, learning rates should be between 106 and 1 and the starting value will be .1 because it is a standard starting value (Bengio 2012). We will tune in both directions from this starting value to find an optimal resolution and speed. This will be the same for both the feed forward network and the RBF network.

\section{Results}

\section{Summary}


\bibliography{report}

\end{document}
